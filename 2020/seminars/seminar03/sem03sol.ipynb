{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wdnw3tE9Deb1"
   },
   "source": [
    "# Neural Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EG1IgkJFLSnW"
   },
   "source": [
    "## Matrix factorization algorithm\n",
    "\n",
    "NCF - это нейронная модель матричной факторизации, которая объединяет Generalized Matrix Factorization (GMF) и Multi-Layer Perceptron (MLP), объединяя в себе сильные стороны линейности MF и нелинейности MLP для моделирования скрытых структур user-item.\n",
    "\n",
    "Схема архитектуры NCF:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GGfys8qiLTs7"
   },
   "source": [
    "<img src=\"https://recodatasets.blob.core.windows.net/images/NCF.svg?sanitize=true\">\n",
    "\n",
    "На схеме видно, как используются скрытые вектора пользователей и айтемов и как затем объединяются выходы из слоя GMF (слева) и слоя MLP (справа).\n",
    "\n",
    "### 1.1 Модель GMF\n",
    "\n",
    "В ALS, матрицу оценок можно записать как:\n",
    "\n",
    "$$\\hat { r } _ { u , i } = q _ { i } ^ { T } p _ { u }$$\n",
    "\n",
    "GMF представляет собой слой NCF как стандартный выходной слой MF. Поэтому MF может быть легко обобщена и расширена. Например, если мы позволим веса ребер выходно слоя обучаться без общего ограничения - это даст вариант MF, который позволяет варьировать важность скрытых измерений. А если мы будем использовать нелинейную функцию активации, это даст обобщение MF до нелинейной формы, которая может быть более выразительной чем линейная MF модель. GMF может быть записана как:\n",
    "\n",
    "$$\\hat { r } _ { u , i } = a _ { o u t } \\left( h ^ { T } \\left( q _ { i } \\odot p _ { u } \\right) \\right)$$\n",
    "\n",
    "где $\\odot$ - поэлементное произведение векторов, ${a}_{out}$ и ${h}$ обозначают функцию активации и веса ребер выходного слоя соответственно. MF может рассматриваться как частный случай GMF. Интуитивно, если мы используем тождественную функцию для ${a}_{out}$ и выбираем единичный вектор в качестве ${h}$, то мы в точности повторяем модель MF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LHuih2qGLppl"
   },
   "source": [
    "\n",
    "### 1.2 Модель MLP\n",
    "\n",
    "NCF использует два способа при моделировании рейтингов:\n",
    "\n",
    "1) поэлементное произведение векторов,\n",
    "2) конкатенация векторов.\n",
    "\n",
    "Сразу после контатенации скрытых признаков пользователей и айтемов применяется стандартная модель MLP. Это дает возможность наделить модель большим уровнем гибкости и нелинейности для изучения взаимодействий между $p_{u}$ и $q_{i}$. \n",
    "\n",
    "Запишем модель MLP модель более строго:\n",
    "\n",
    "Для входного слоя, используется конкатенация векторов пользователей и айтемов:\n",
    "\n",
    "$$z _ { 1 } = \\phi _ { 1 } \\left( p _ { u } , q _ { i } \\right) = \\left[ \\begin{array} { c } { p _ { u } } \\\\ { q _ { i } } \\end{array} \\right]$$\n",
    "\n",
    "Для скрытых и выходного слоев MLP запись имеет вид:\n",
    "\n",
    "$$\n",
    "\\phi _ { l } \\left( z _ { l } \\right) = a _ { o u t } \\left( W _ { l } ^ { T } z _ { l } + b _ { l } \\right) , ( l = 2,3 , \\ldots , L - 1 )\n",
    "$$\n",
    "\n",
    "и:\n",
    "\n",
    "$$\n",
    "\\hat { r } _ { u , i } = \\sigma \\left( h ^ { T } \\phi \\left( z _ { L - 1 } \\right) \\right)\n",
    "$$\n",
    "\n",
    "где ${ W }_{ l }$, ${ b }_{ l }$, и ${ a }_{ out }$ обозначают матрицу весов, вектор свободных членов и функцию активации для $l$-ого слоя, соответственно. В качестве функций активации MLP слоев, мы вольны выбирать любую: сигмоиду, гиперболический тангенс, ReLU и другие. В качестве функции активации на выходном слое используется сигмоида $\\sigma(x)=\\frac{1}{1+e^{-x}}$, чтобы ограничить оценки диапазоном (0,1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-SmWKJn1Lt7y"
   },
   "source": [
    "### 1.3 Смешивание GMF и MLP\n",
    "\n",
    "Чтобы обеспечить большую гибкость нашей смешанной модели мы позволяем GMF и MLP обучать независимые эмбединги и затем комбинируем две модели объединяя их последние скрытие слои. Мы взяли $\\phi^{GMF}$ из GMF:\n",
    "\n",
    "$$\\phi _ { u , i } ^ { G M F } = p _ { u } ^ { G M F } \\odot q _ { i } ^ { G M F }$$\n",
    "\n",
    "и получили $\\phi^{MLP}$ из MLP:\n",
    "\n",
    "$$\\phi _ { u , i } ^ { M L P } = a _ { o u t } \\left( W _ { L } ^ { T } \\left( a _ { o u t } \\left( \\ldots a _ { o u t } \\left( W _ { 2 } ^ { T } \\left[ \\begin{array} { c } { p _ { u } ^ { M L P } } \\\\ { q _ { i } ^ { M L P } } \\end{array} \\right] + b _ { 2 } \\right) \\ldots \\right) \\right) + b _ { L }\\right.$$\n",
    "\n",
    "Наконец, мы смешали выходы из GMF и MLP:\n",
    "\n",
    "$$\\hat { r } _ { u , i } = \\sigma \\left( h ^ { T } \\left[ \\begin{array} { l } { \\phi ^ { G M F } } \\\\ { \\phi ^ { M L P } } \\end{array} \\right] \\right)$$\n",
    "\n",
    "Модель сочетает линейность MF и нелинейность DNN при моделировании скрытых user–item структур."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SDJwiFk8LwXr"
   },
   "source": [
    "### 1.4 Целевая функция\n",
    "\n",
    "Мы можем записать функцию правдоподобия как:\n",
    "\n",
    "$$P \\left( \\mathcal { R } , \\mathcal { R } ^ { - } | \\mathbf { P } , \\mathbf { Q } , \\Theta \\right) = \\prod _ { ( u , i ) \\in \\mathcal { R } } \\hat { r } _ { u , i } \\prod _ { ( u , j ) \\in \\mathcal { R } ^{ - } } \\left( 1 - \\hat { r } _ { u , j } \\right)$$\n",
    "\n",
    "Где $\\mathcal{R}$ обозначает множество наблюдаемых взаимодействий пользователя, а $\\mathcal{ R } ^ { - }$ обозначает множество негативных наблюдений. $\\mathbf{P}$ и $\\mathbf{Q}$ - это скрытая матрица признаков пользователей и айтемов соответственно, $\\Theta$ - параметры модели. Взяв со знаком минус логарифм от правдоподобия мы получим целевую функцию для минимизации NCF алгоритма. Что-то напоминает, не правда ли? https://en.wikipedia.org/wiki/Cross_entropy\n",
    "\n",
    "$$L = - \\sum _ { ( u , i ) \\in \\mathcal { R } \\cup { \\mathcal { R } } ^ { - } } r _ { u , i } \\log \\hat { r } _ { u , i } + \\left( 1 - r _ { u , i } \\right) \\log \\left( 1 - \\hat { r } _ { u , i } \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zAyBExMGLWJC"
   },
   "source": [
    "## Tensorflow NCF implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1YdVsP6LElUF"
   },
   "source": [
    "### Downloading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 590
    },
    "colab_type": "code",
    "id": "SyKykR2fEqDB",
    "outputId": "72c462b5-4077-44cb-ba84-985e2fc41c75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-02-06 07:03:22--  https://raw.githubusercontent.com/hexiangnan/neural_collaborative_filtering/master/Data/ml-1m.test.negative\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2891424 (2.8M) [text/plain]\n",
      "Saving to: ‘ml-1m.test.negative’\n",
      "\n",
      "\r",
      "ml-1m.test.negative   0%[                    ]       0  --.-KB/s               \r",
      "ml-1m.test.negative 100%[===================>]   2.76M  --.-KB/s    in 0.05s   \n",
      "\n",
      "2020-02-06 07:03:24 (52.3 MB/s) - ‘ml-1m.test.negative’ saved [2891424/2891424]\n",
      "\n",
      "--2020-02-06 07:03:27--  https://raw.githubusercontent.com/hexiangnan/neural_collaborative_filtering/master/Data/ml-1m.test.rating\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 128039 (125K) [text/plain]\n",
      "Saving to: ‘ml-1m.test.rating’\n",
      "\n",
      "ml-1m.test.rating   100%[===================>] 125.04K  --.-KB/s    in 0.004s  \n",
      "\n",
      "2020-02-06 07:03:27 (31.6 MB/s) - ‘ml-1m.test.rating’ saved [128039/128039]\n",
      "\n",
      "--2020-02-06 07:03:29--  https://raw.githubusercontent.com/hexiangnan/neural_collaborative_filtering/master/Data/ml-1m.train.rating\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 20982911 (20M) [text/plain]\n",
      "Saving to: ‘ml-1m.train.rating’\n",
      "\n",
      "ml-1m.train.rating  100%[===================>]  20.01M  --.-KB/s    in 0.1s    \n",
      "\n",
      "2020-02-06 07:03:31 (153 MB/s) - ‘ml-1m.train.rating’ saved [20982911/20982911]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/hexiangnan/neural_collaborative_filtering/master/Data/ml-1m.test.negative\n",
    "!wget https://raw.githubusercontent.com/hexiangnan/neural_collaborative_filtering/master/Data/ml-1m.test.rating\n",
    "!wget https://raw.githubusercontent.com/hexiangnan/neural_collaborative_filtering/master/Data/ml-1m.train.rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D3a-EgDuDYUw"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "_UL1pN8cfG4j",
    "outputId": "5333f220-4c00-4ceb-9998-b4f8929188ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow 2.x selected.\n",
      "TF version: 2.1.0\n",
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "import heapq\n",
    "import math\n",
    "import multiprocessing\n",
    "import sys\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from six.moves import xrange\n",
    "\n",
    "%tensorflow_version 2.x\n",
    "import tensorflow as tf\n",
    "\n",
    "print(\"TF version:\", tf.__version__)\n",
    "print(\n",
    "    \"GPU is\", \"available\" if tf.config.list_physical_devices(\"GPU\") else \"NOT AVAILABLE\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "32-nxuu6Db6M"
   },
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ORrCQPT4uZgl"
   },
   "outputs": [],
   "source": [
    "# ml-1m dataset contains 1,000,209 anonymous ratings of approximately 3,706 movies made by 6,040 users who joined MovieLens in 2000.\n",
    "# All ratings are contained in the file \"ratings.dat\" without header row, and are in the following format:\n",
    "# UserID::MovieID::Rating::Timestamp\n",
    "#\n",
    "# - UserIDs range between 1 and 6040.\n",
    "# - MovieIDs range between 1 and 3952.\n",
    "# - Ratings are made on a 5-star scale (whole-star ratings only).\n",
    "\n",
    "FILE_NAME = \"ml-1m\"\n",
    "\n",
    "USER_COLUMN = \"user_id\"\n",
    "ITEM_COLUMN = \"item_id\"  # movies\n",
    "RATING_COLUMN = \"rating\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mSNns4ibJCj6"
   },
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C7hN0q1KZFGw"
   },
   "source": [
    "Данные предварительно предобработаны:\n",
    "\n",
    "**train.rating:**\n",
    "- Train file.\n",
    "- Each Line is a training instance: userID itemID rating timestamp (if have)\n",
    "\n",
    "**test.rating:**\n",
    "- Test file (positive instances). \n",
    "- Each Line is a testing instance: userID itemID rating timestamp (if have)\n",
    "\n",
    "**test.negative**\n",
    "- Test file (negative instances).\n",
    "- Each line corresponds to the line of test.rating, containing 99 negative samples.  \n",
    "- Each line is in the format: (userID,itemID) negativeItemID1 negativeItemID2 ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B_6RifGb_rI3"
   },
   "outputs": [],
   "source": [
    "### Ничего интересного - чтение данных из файлов (можно пропустить).\n",
    "\n",
    "\n",
    "def load_rating_file_as_list(filename):\n",
    "    ratingList = []\n",
    "    with open(filename + \".test.rating\", \"r\") as f:\n",
    "        line = f.readline()\n",
    "        while line != None and line != \"\":\n",
    "            arr = line.split(\"\\t\")\n",
    "            user, item = int(arr[0]), int(arr[1])\n",
    "            ratingList.append([user, item])\n",
    "            line = f.readline()\n",
    "    return ratingList\n",
    "\n",
    "\n",
    "def load_negative_file(filename):\n",
    "    negativeList = []\n",
    "    with open(filename + \".test.negative\", \"r\") as f:\n",
    "        line = f.readline()\n",
    "        while line != None and line != \"\":\n",
    "            arr = line.split(\"\\t\")\n",
    "            negatives = []\n",
    "            for x in arr[1:]:\n",
    "                negatives.append(int(x))\n",
    "            negativeList.append(negatives)\n",
    "            line = f.readline()\n",
    "    return negativeList\n",
    "\n",
    "\n",
    "def load_rating_file_as_matrix(filename):\n",
    "    \"\"\"\n",
    "    Read .rating file and Return dok matrix.\n",
    "    \"\"\"\n",
    "    # Get number of users and items\n",
    "    num_users, num_items = 0, 0\n",
    "    with open(filename + \".train.rating\", \"r\") as f:\n",
    "        line = f.readline()\n",
    "        while line != None and line != \"\":\n",
    "            arr = line.split(\"\\t\")\n",
    "            u, i = int(arr[0]), int(arr[1])\n",
    "            num_users = max(num_users, u)\n",
    "            num_items = max(num_items, i)\n",
    "            line = f.readline()\n",
    "    # Construct matrix\n",
    "    mat = sp.dok_matrix((num_users + 1, num_items + 1), dtype=np.float32)\n",
    "    with open(filename + \".train.rating\", \"r\") as f:\n",
    "        line = f.readline()\n",
    "        while line != None and line != \"\":\n",
    "            arr = line.split(\"\\t\")\n",
    "            user, item, rating = int(arr[0]), int(arr[1]), float(arr[2])\n",
    "            if rating > 0:\n",
    "                mat[user, item] = 1.0\n",
    "            line = f.readline()\n",
    "    return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "MZE5OBXGAdyz",
    "outputId": "31160392-52aa-4f88-f48d-bd60b4315ef5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data done [13.2 s]. #user=6040, #item=3706, #train=994169, #test=6040\n"
     ]
    }
   ],
   "source": [
    "t1 = time()\n",
    "\n",
    "# Loading data\n",
    "train, testRatings, testNegatives = (\n",
    "    load_rating_file_as_matrix(FILE_NAME),\n",
    "    load_rating_file_as_list(FILE_NAME),\n",
    "    load_negative_file(FILE_NAME),\n",
    ")\n",
    "num_users, num_items = train.shape\n",
    "\n",
    "print(\n",
    "    \"Load data done [%.1f s]. #user=%d, #item=%d, #train=%d, #test=%d\"\n",
    "    % (time() - t1, num_users, num_items, train.nnz, len(testRatings))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "134bmv7QEDhR"
   },
   "source": [
    "### Metrics functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nsjzYANX9cJ6"
   },
   "outputs": [],
   "source": [
    "### Расчет метрик: Hit_Ratio, NDCG для top-K рекомендаций\n",
    "\n",
    "# Global variables that are shared across processes\n",
    "_model = None\n",
    "_testRatings = None\n",
    "_testNegatives = None\n",
    "_K = None\n",
    "\n",
    "\n",
    "def getHitRatio(ranklist, gtItem):\n",
    "    for item in ranklist:\n",
    "        if item == gtItem:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "def getNDCG(ranklist, gtItem):\n",
    "    for i in xrange(len(ranklist)):\n",
    "        item = ranklist[i]\n",
    "        if item == gtItem:\n",
    "            return math.log(2) / math.log(i + 2)\n",
    "    return 0\n",
    "\n",
    "\n",
    "def eval_one_rating(idx):\n",
    "    rating = _testRatings[idx]\n",
    "    items = _testNegatives[idx]\n",
    "    u = rating[0]\n",
    "    gtItem = rating[1]\n",
    "    items.append(gtItem)\n",
    "    # Get prediction scores\n",
    "    map_item_score = {}\n",
    "    users = np.full(len(items), u, dtype=\"int32\")\n",
    "    predictions = _model.predict([users, np.array(items)], batch_size=100, verbose=0)\n",
    "    for i in xrange(len(items)):\n",
    "        item = items[i]\n",
    "        map_item_score[item] = predictions[i]\n",
    "    items.pop()\n",
    "\n",
    "    # Evaluate top rank list\n",
    "    ranklist = heapq.nlargest(_K, map_item_score, key=map_item_score.get)\n",
    "    hr = getHitRatio(ranklist, gtItem)\n",
    "    ndcg = getNDCG(ranklist, gtItem)\n",
    "    return (hr, ndcg)\n",
    "\n",
    "\n",
    "def evaluate_model(model, testRatings, testNegatives, K, num_thread):\n",
    "\n",
    "    global _model\n",
    "    global _testRatings\n",
    "    global _testNegatives\n",
    "    global _K\n",
    "    _model = model\n",
    "    _testRatings = testRatings\n",
    "    _testNegatives = testNegatives\n",
    "    _K = K\n",
    "\n",
    "    hits, ndcgs = [], []\n",
    "    if num_thread > 1:  # Multi-thread\n",
    "        pool = multiprocessing.Pool(processes=num_thread)\n",
    "        res = pool.map(eval_one_rating, range(len(_testRatings)))\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        hits = [r[0] for r in res]\n",
    "        ndcgs = [r[1] for r in res]\n",
    "        return (hits, ndcgs)\n",
    "    # Single thread\n",
    "    for idx in xrange(len(_testRatings)):\n",
    "        (hr, ndcg) = eval_one_rating(idx)\n",
    "        hits.append(hr)\n",
    "        ndcgs.append(ndcg)\n",
    "    return (hits, ndcgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bQC7ggbEJXSo"
   },
   "source": [
    "### Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2eCKo2fnfsFs"
   },
   "outputs": [],
   "source": [
    "mf_regularization = 0.0  # regularization factor for MF embeddings\n",
    "\n",
    "\n",
    "def get_model(\n",
    "    num_users, num_items, mf_dim=10, model_layers=[100], mlp_reg_layers=[0.0], reg_mf=0\n",
    "):\n",
    "    assert len(model_layers) == len(mlp_reg_layers)\n",
    "\n",
    "    # Input variables\n",
    "\n",
    "    user_input = tf.keras.layers.Input(shape=(1,), name=USER_COLUMN, dtype=tf.int32)\n",
    "\n",
    "    item_input = tf.keras.layers.Input(shape=(1,), name=ITEM_COLUMN, dtype=tf.int32)\n",
    "\n",
    "    # Embedding layer\n",
    "\n",
    "    # Initializer for embedding layers\n",
    "    embedding_initializer = \"glorot_uniform\"\n",
    "\n",
    "    # It turns out to be significantly more effecient to store the MF and MLP\n",
    "    # embedding portions in the same table, and then slice as needed.\n",
    "    embedding_user = tf.keras.layers.Embedding(\n",
    "        num_users,\n",
    "        mf_dim + model_layers[0] // 2,\n",
    "        embeddings_initializer=embedding_initializer,\n",
    "        embeddings_regularizer=tf.keras.regularizers.l2(mf_regularization),\n",
    "        input_length=1,\n",
    "        name=\"embedding_user\",\n",
    "    )(user_input)\n",
    "\n",
    "    embedding_item = tf.keras.layers.Embedding(\n",
    "        num_items,\n",
    "        mf_dim + model_layers[0] // 2,\n",
    "        embeddings_initializer=embedding_initializer,\n",
    "        embeddings_regularizer=tf.keras.regularizers.l2(mf_regularization),\n",
    "        input_length=1,\n",
    "        name=\"embedding_item\",\n",
    "    )(item_input)\n",
    "\n",
    "    # GMF part\n",
    "\n",
    "    def mf_slice_fn(x):\n",
    "        x = tf.squeeze(x, [1])\n",
    "        return x[:, :mf_dim]\n",
    "\n",
    "    mf_user_latent = tf.keras.layers.Lambda(mf_slice_fn, name=\"embedding_user_mf\")(\n",
    "        embedding_user\n",
    "    )\n",
    "    mf_item_latent = tf.keras.layers.Lambda(mf_slice_fn, name=\"embedding_item_mf\")(\n",
    "        embedding_item\n",
    "    )\n",
    "\n",
    "    # Element-wise multiply\n",
    "    mf_vector = tf.keras.layers.multiply([mf_user_latent, mf_item_latent])\n",
    "\n",
    "    # MLP part\n",
    "\n",
    "    def mlp_slice_fn(x):\n",
    "        x = tf.squeeze(x, [1])\n",
    "        return x[:, mf_dim:]\n",
    "\n",
    "    mlp_user_latent = tf.keras.layers.Lambda(mlp_slice_fn, name=\"embedding_user_mlp\")(\n",
    "        embedding_user\n",
    "    )\n",
    "    mlp_item_latent = tf.keras.layers.Lambda(mlp_slice_fn, name=\"embedding_item_mlp\")(\n",
    "        embedding_item\n",
    "    )\n",
    "\n",
    "    # Concatenation of two latent features\n",
    "    mlp_vector = tf.keras.layers.concatenate([mlp_user_latent, mlp_item_latent])\n",
    "\n",
    "    num_layer = len(model_layers)  # Number of layers in the MLP\n",
    "    for layer in xrange(1, num_layer):\n",
    "        model_layer = tf.keras.layers.Dense(\n",
    "            model_layers[layer],\n",
    "            kernel_regularizer=tf.keras.regularizers.l2(mlp_reg_layers[layer]),\n",
    "            activation=\"relu\",\n",
    "        )\n",
    "        mlp_vector = model_layer(mlp_vector)\n",
    "\n",
    "    # Concatenate GMF and MLP parts\n",
    "    predict_vector = tf.keras.layers.concatenate([mf_vector, mlp_vector])\n",
    "\n",
    "    # Final prediction layer\n",
    "    logits = tf.keras.layers.Dense(\n",
    "        1, activation=None, kernel_initializer=\"lecun_uniform\", name=RATING_COLUMN\n",
    "    )(predict_vector)\n",
    "\n",
    "    model = tf.keras.models.Model([user_input, item_input], logits)\n",
    "\n",
    "    # Print model topology.\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 625
    },
    "colab_type": "code",
    "id": "YaZg7_TH7FT8",
    "outputId": "a96d491f-2bdc-4dc4-f38b-11b5019c6ec4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "user_id (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "item_id (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_user (Embedding)      (None, 1, 60)        362400      user_id[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_item (Embedding)      (None, 1, 60)        222360      item_id[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_user_mf (Lambda)      (None, 10)           0           embedding_user[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "embedding_item_mf (Lambda)      (None, 10)           0           embedding_item[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "embedding_user_mlp (Lambda)     (None, 50)           0           embedding_user[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "embedding_item_mlp (Lambda)     (None, 50)           0           embedding_item[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "multiply_2 (Multiply)           (None, 10)           0           embedding_user_mf[0][0]          \n",
      "                                                                 embedding_item_mf[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 100)          0           embedding_user_mlp[0][0]         \n",
      "                                                                 embedding_item_mlp[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 110)          0           multiply_2[0][0]                 \n",
      "                                                                 concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "rating (Dense)                  (None, 1)            111         concatenate_5[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 584,871\n",
      "Trainable params: 584,871\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_model(num_users, num_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "blhjtveG7Rhn"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=0.1), loss=\"binary_crossentropy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I17ntIP8OdEP"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "K9SIDjZWA8so",
    "outputId": "2460d8d2-897b-428a-85a4-a048db66548a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init: HR = 0.1071, NDCG = 0.0499\n",
      "Took 131.8 seconds for training.\n"
     ]
    }
   ],
   "source": [
    "# обучается ~ 2 мин.\n",
    "start_time = time()\n",
    "\n",
    "# Init performance\n",
    "topK = 10\n",
    "evaluation_threads = 1  # multiprocessing.cpu_count()\n",
    "\n",
    "(hits, ndcgs) = evaluate_model(\n",
    "    model, testRatings, testNegatives, topK, evaluation_threads\n",
    ")\n",
    "hr, ndcg = np.array(hits).mean(), np.array(ndcgs).mean()\n",
    "print(\"Init: HR = %.4f, NDCG = %.4f\" % (hr, ndcg))\n",
    "\n",
    "train_time = time() - start_time\n",
    "print(\"Took %.1f seconds for training.\" % (train_time))\n",
    "\n",
    "best_hr, best_ndcg, best_iter = hr, ndcg, -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AVSPsbGNPyND"
   },
   "outputs": [],
   "source": [
    "def get_train_instances(train, num_negatives):\n",
    "    user_input, item_input, labels = [], [], []\n",
    "    num_users = train.shape[0]\n",
    "    for (u, i) in train.keys():\n",
    "        # positive instance\n",
    "        user_input.append(u)\n",
    "        item_input.append(i)\n",
    "        labels.append(1)\n",
    "        # negative instances\n",
    "        for t in xrange(num_negatives):\n",
    "            j = np.random.randint(num_items)\n",
    "            while (u, j) in train:\n",
    "                j = np.random.randint(num_items)\n",
    "            user_input.append(u)\n",
    "            item_input.append(j)\n",
    "            labels.append(0)\n",
    "    return user_input, item_input, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "HjN_V5CH75P2",
    "outputId": "f44dc625-3f12-42b9-f842-c45212766074"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 [279.8 s]: HR = 0.1409, NDCG = 0.0690, loss = 1.3986 [133.2 s]\n",
      "End. Best Iteration 0:  HR = 0.1409, NDCG = 0.0690. \n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1  # 20\n",
    "num_negatives = 10\n",
    "batch_size = 256\n",
    "verbose = 1\n",
    "\n",
    "# Training model\n",
    "for epoch in xrange(num_epochs):\n",
    "    t1 = time()\n",
    "    # Generate training instances\n",
    "    user_input, item_input, labels = get_train_instances(train, num_negatives)\n",
    "\n",
    "    # Training\n",
    "    hist = model.fit(\n",
    "        [np.array(user_input), np.array(item_input)],  # input\n",
    "        np.array(labels),  # labels\n",
    "        batch_size=batch_size,\n",
    "        epochs=1,\n",
    "        verbose=0,\n",
    "        shuffle=True,\n",
    "    )\n",
    "    t2 = time()\n",
    "\n",
    "    # Evaluation\n",
    "    if epoch % verbose == 0:\n",
    "        (hits, ndcgs) = evaluate_model(\n",
    "            model, testRatings, testNegatives, topK, evaluation_threads\n",
    "        )\n",
    "        hr, ndcg, loss = (\n",
    "            np.array(hits).mean(),\n",
    "            np.array(ndcgs).mean(),\n",
    "            hist.history[\"loss\"][0],\n",
    "        )\n",
    "        print(\n",
    "            \"Iteration %d [%.1f s]: HR = %.4f, NDCG = %.4f, loss = %.4f [%.1f s]\"\n",
    "            % (epoch, t2 - t1, hr, ndcg, loss, time() - t2)\n",
    "        )\n",
    "        if hr > best_hr:\n",
    "            best_hr, best_ndcg, best_iter = hr, ndcg, epoch\n",
    "\n",
    "print(\n",
    "    \"End. Best Iteration %d:  HR = %.4f, NDCG = %.4f. \"\n",
    "    % (best_iter, best_hr, best_ndcg)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vMLt_wYeMQ3f"
   },
   "source": [
    "## Полезные ссылки:\n",
    "\n",
    "### Статья, на который все ссылаются:\n",
    "Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu & Tat-Seng Chua, Neural Collaborative Filtering, 2017 https://arxiv.org/abs/1708.05031\n",
    "\n",
    "### Туториалы и реализации:\n",
    "##### Microsoft:\n",
    "https://github.com/microsoft/recommenders/blob/master/notebooks/02_model/ncf_deep_dive.ipynb\n",
    "\n",
    "##### Tensorflow:\n",
    "https://github.com/tensorflow/models/tree/master/official/recommendation\n",
    "\n",
    "##### Towards Data Science\n",
    "https://towardsdatascience.com/neural-collaborative-filtering-96cef1009401\n",
    "\n",
    "### Лекции:\n",
    "Е.Соколов. Рекомендательные системы. Лекция 1 https://github.com/hse-ds/iad-applied-ds/blob/master/2020/lectures/lecture01-recommender.pdf\n",
    "\n",
    "Е.Соколов. Рекомендательные системы. Лекция 2 https://github.com/hse-ds/iad-applied-ds/blob/master/2020/lectures/lecture02-recommender.pdf\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "NCF_full",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
