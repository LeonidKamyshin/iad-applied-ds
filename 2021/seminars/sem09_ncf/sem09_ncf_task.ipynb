{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Прикладные задачи анализа данных</center></h1>\n",
    "<h2><center>Семинар: рекомендательные системы - 2</center></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Рекомендация фильмов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом семинаре рассмотрим рекомендательную систему фильмов на основе нейронных сетей. Будем работать с данными, которые сожержат 1,000,209 анонимных рейтингов для примерно 3,900 фильмов от 6,040 пользователей MovieLens, которые присоединись в 2000.\n",
    "\n",
    "<center><img src=\"https://i.imgflip.com/1pvf4b.jpg\" width=\"400\"></center>\n",
    "\n",
    "**Source:** https://grouplens.org/datasets/movielens/1m/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# скачаем данные\n",
    "!wget https://raw.githubusercontent.com/gaoxx643/MovieLens-1M-Dataset/master/movies.dat\n",
    "!wget https://raw.githubusercontent.com/gaoxx643/MovieLens-1M-Dataset/master/users.dat\n",
    "!wget https://raw.githubusercontent.com/gaoxx643/MovieLens-1M-Dataset/master/ratings.dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MOVIES FILE DESCRIPTION\n",
    "\n",
    "Movie information is in the file \"movies.dat\" and is in the following\n",
    "format:\n",
    "\n",
    "MovieID::Title::Genres\n",
    "\n",
    "- Titles are identical to titles provided by the IMDB (including\n",
    "year of release)\n",
    "- Genres are pipe-separated and are selected from the following genres:\n",
    "\n",
    "    * Action\n",
    "    * Adventure\n",
    "    * Animation\n",
    "    * Children's\n",
    "    * Comedy\n",
    "    * Crime\n",
    "    * Documentary\n",
    "    * Drama\n",
    "    * Fantasy\n",
    "    * Film-Noir\n",
    "    * Horror\n",
    "    * Musical\n",
    "    * Mystery\n",
    "    * Romance\n",
    "    * Sci-Fi\n",
    "    * Thriller\n",
    "    * War\n",
    "    * Western\n",
    "\n",
    "- Some MovieIDs do not correspond to a movie due to accidental duplicate\n",
    "entries and/or test entries\n",
    "- Movies are mostly entered by hand, so errors and inconsistencies may exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = pd.read_csv(\"movies.dat\", sep=\"::\", engine=\"python\", header=None)\n",
    "movies.columns = [\"MovieID\", \"Title\", \"Genres\"]\n",
    "movies.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### USERS FILE DESCRIPTION\n",
    "\n",
    "User information is in the file \"users.dat\" and is in the following\n",
    "format:\n",
    "\n",
    "UserID::Gender::Age::Occupation::Zip-code\n",
    "\n",
    "All demographic information is provided voluntarily by the users and is\n",
    "not checked for accuracy.  Only users who have provided some demographic\n",
    "information are included in this data set.\n",
    "\n",
    "- Gender is denoted by a \"M\" for male and \"F\" for female\n",
    "- Age is chosen from the following ranges:\n",
    "\n",
    "    *  1:  \"Under 18\"\n",
    "    * 18:  \"18-24\"\n",
    "    * 25:  \"25-34\"\n",
    "    * 35:  \"35-44\"\n",
    "    * 45:  \"45-49\"\n",
    "    * 50:  \"50-55\"\n",
    "    * 56:  \"56+\"\n",
    "\n",
    "- Occupation is chosen from the following choices:\n",
    "\n",
    "    *  0:  \"other\" or not specified\n",
    "    *  1:  \"academic/educator\"\n",
    "    *  2:  \"artist\"\n",
    "    *  3:  \"clerical/admin\"\n",
    "    *  4:  \"college/grad student\"\n",
    "    *  5:  \"customer service\"\n",
    "    *  6:  \"doctor/health care\"\n",
    "    *  7:  \"executive/managerial\"\n",
    "    *  8:  \"farmer\"\n",
    "    *  9:  \"homemaker\"\n",
    "    * 10:  \"K-12 student\"\n",
    "    * 11:  \"lawyer\"\n",
    "    * 12:  \"programmer\"\n",
    "    * 13:  \"retired\"\n",
    "    * 14:  \"sales/marketing\"\n",
    "    * 15:  \"scientist\"\n",
    "    * 16:  \"self-employed\"\n",
    "    * 17:  \"technician/engineer\"\n",
    "    * 18:  \"tradesman/craftsman\"\n",
    "    * 19:  \"unemployed\"\n",
    "    * 20:  \"writer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = pd.read_csv(\"users.dat\", sep=\"::\", engine=\"python\", header=None)\n",
    "users.columns = [\"UserID\", \"Gender\", \"Age\", \"Occupation\", \"Zip-code\"]\n",
    "users.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RATINGS FILE DESCRIPTION\n",
    "\n",
    "All ratings are contained in the file \"ratings.dat\" and are in the\n",
    "following format:\n",
    "\n",
    "UserID::MovieID::Rating::Timestamp\n",
    "\n",
    "- UserIDs range between 1 and 6040 \n",
    "- MovieIDs range between 1 and 3952\n",
    "- Ratings are made on a 5-star scale (whole-star ratings only)\n",
    "- Timestamp is represented in seconds since the epoch as returned by time(2)\n",
    "- Each user has at least 20 ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = pd.read_csv(\"ratings.dat\", sep=\"::\", engine=\"python\", header=None)\n",
    "ratings.columns = [\"UserID\", \"MovieID\", \"Rating\", \"Timestamp\"]\n",
    "ratings.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Предобработанные данные\n",
    "\n",
    "Для построения нашей рекомендательной системы мы будем использовать уже подготовленные данные. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# загружаем данные\n",
    "!wget https://raw.githubusercontent.com/hexiangnan/neural_collaborative_filtering/master/Data/ml-1m.test.negative\n",
    "!wget https://raw.githubusercontent.com/hexiangnan/neural_collaborative_filtering/master/Data/ml-1m.test.rating\n",
    "!wget https://raw.githubusercontent.com/hexiangnan/neural_collaborative_filtering/master/Data/ml-1m.train.rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данные предварительно предобработаны:\n",
    "\n",
    "**train.rating:**\n",
    "- Train file.\n",
    "- Each Line is a training instance: userID itemID rating timestamp (if have)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\n",
    "    \"ml-1m.train.rating\",\n",
    "    sep=\"\\t\",\n",
    "    header=None,\n",
    "    names=[\"user\", \"item\", \"rating\", \"timestamp\"],\n",
    "    usecols=[0, 1, 2, 3],\n",
    "    dtype={0: np.int32, 1: np.int32, 2: np.int32, 3: np.int32},\n",
    ")\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_num = train_data[\"user\"].max() + 1\n",
    "item_num = train_data[\"item\"].max() + 1\n",
    "\n",
    "user_num, item_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data[[\"user\", \"item\"]].values.tolist()\n",
    "train_data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Матрица ретингов: строки - пользователи, столбцы - фильмы, на пересечении - 0 (не смотрел фильм) или 1 (смотрел)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "\n",
    "# load ratings as a dok matrix\n",
    "train_mat = sp.dok_matrix((user_num, item_num), dtype=np.float32)\n",
    "for x in train_data:\n",
    "    train_mat[x[0], x[1]] = 1.0\n",
    "\n",
    "# train_mat[userID, itemID] = 0 or 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**test.rating:**\n",
    "- Test file (positive instances). \n",
    "- Each Line is a testing instance: userID itemID rating timestamp (if have)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(\n",
    "    \"ml-1m.test.rating\",\n",
    "    sep=\"\\t\",\n",
    "    header=None,\n",
    "    names=[\"user\", \"item\", \"rating\", \"timestamp\"],\n",
    "    usecols=[0, 1, 2, 3],\n",
    "    dtype={0: np.int32, 1: np.int32, 2: np.int32, 3: np.int32},\n",
    ")\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**test.negative**\n",
    "- Test file (negative instances).\n",
    "- Each line corresponds to the line of test.rating, containing 99 negative samples.  \n",
    "- Each line is in the format: (userID,itemID) negativeItemID1 negativeItemID2 ...\n",
    "\n",
    "Это подготовленный набор данных, который мы будем использовать для тестирования нашей рекомендательной системы. Для каждого пользователя он содержит 1 фильм, который он сомтрел, и 99 фильмов, которые он НЕ смотрел. Мы будем использовать их для подсчета метрик качества."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = []\n",
    "with open(\"ml-1m.test.negative\", \"r\") as fd:\n",
    "    line = fd.readline()\n",
    "    while line != None and line != \"\":\n",
    "        arr = line.split(\"\\t\")\n",
    "        u = eval(arr[0])[0]\n",
    "        test_data.append([u, eval(arr[0])[1]])\n",
    "        for i in arr[1:]:\n",
    "            test_data.append([u, int(i)])\n",
    "        line = fd.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подготовка dataloaders\n",
    "\n",
    "Реализуем специальный класс для работы с данными. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NCFData(data.Dataset):\n",
    "    def __init__(self, features, num_item, train_mat=None, num_ng=0, is_training=None):\n",
    "        super(NCFData, self).__init__()\n",
    "\n",
    "        \"\"\" Note that the labels are only useful when training, we thus \n",
    "            add them in the ng_sample() function.\n",
    "        \"\"\"\n",
    "\n",
    "        self.features_ps = features\n",
    "        self.num_item = num_item\n",
    "        self.train_mat = train_mat\n",
    "        self.num_ng = num_ng\n",
    "        self.is_training = is_training\n",
    "        self.labels = [0 for _ in range(len(features))]\n",
    "\n",
    "    def ng_sample(self):\n",
    "        # добавляем фильмы, которые пользователь не смотрел.\n",
    "        # это нужно для примеров 0-го класса при обучении.\n",
    "\n",
    "        assert self.is_training, \"no need to sampling when testing\"\n",
    "\n",
    "        self.features_ng = []\n",
    "        for x in self.features_ps:\n",
    "            u = x[0]\n",
    "            for t in range(self.num_ng):\n",
    "                j = np.random.randint(self.num_item)\n",
    "                while (u, j) in self.train_mat:\n",
    "                    j = np.random.randint(self.num_item)\n",
    "                self.features_ng.append([u, j])\n",
    "\n",
    "        # тру метрки классов\n",
    "        labels_ps = [1 for _ in range(len(self.features_ps))]\n",
    "        labels_ng = [0 for _ in range(len(self.features_ng))]\n",
    "\n",
    "        self.features_fill = self.features_ps + self.features_ng\n",
    "        self.labels_fill = labels_ps + labels_ng\n",
    "\n",
    "    def __len__(self):\n",
    "        return (self.num_ng + 1) * len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        features = self.features_fill if self.is_training else self.features_ps\n",
    "        labels = self.labels_fill if self.is_training else self.labels\n",
    "\n",
    "        user = features[idx][0]\n",
    "        item = features[idx][1]\n",
    "        label = labels[idx]  # Нужны только для обучения. В тесте не используются.\n",
    "        return user, item, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = NCFData(train_data, item_num, train_mat, num_ng=1, is_training=True)\n",
    "test_dataset = NCFData(test_data, item_num, train_mat, num_ng=0, is_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# попросим класс добавить негативный примеров (фильмы. которые не смотрел пользователь)\n",
    "train_dataset.ng_sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# пример использования\n",
    "print(\"User, Item, Label\")\n",
    "\n",
    "for idx, (user, item, label) in enumerate(train_dataset):\n",
    "    print(user, item, label)\n",
    "    if idx > 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создадим лоадеры для pytorch\n",
    "train_loader = data.DataLoader(\n",
    "    train_dataset, batch_size=256, shuffle=True, num_workers=4\n",
    ")\n",
    "test_loader = data.DataLoader(\n",
    "    test_dataset, batch_size=100, shuffle=False, num_workers=0\n",
    ")  # one user per batch !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Collaborative Filtering (NCF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix factorization algorithm\n",
    "\n",
    "NCF - это нейронная модель матричной факторизации, которая объединяет Generalized Matrix Factorization (GMF) и Multi-Layer Perceptron (MLP), объединяя в себе сильные стороны линейности MF и нелинейности MLP для моделирования скрытых структур user-item.\n",
    "\n",
    "Схема архитектуры NCF:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://recodatasets.blob.core.windows.net/images/NCF.svg?sanitize=true\">\n",
    "\n",
    "На схеме видно, как используются скрытые вектора пользователей и айтемов и как затем объединяются выходы из слоя GMF (слева) и слоя MLP (справа).\n",
    "\n",
    "**Статья:** Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu & Tat-Seng Chua, Neural Collaborative Filtering, 2017 https://arxiv.org/abs/1708.05031\n",
    "\n",
    "### 1 Модель GMF\n",
    "\n",
    "В ALS, матрицу оценок можно записать как:\n",
    "\n",
    "$$\\hat { r } _ { u , i } = q _ { i } ^ { T } p _ { u }$$\n",
    "\n",
    "GMF представляет собой слой NCF как стандартный выходной слой MF. Поэтому MF может быть легко обобщена и расширена. Например, если мы позволим веса ребер выходно слоя обучаться без общего ограничения - это даст вариант MF, который позволяет варьировать важность скрытых измерений. А если мы будем использовать нелинейную функцию активации, это даст обобщение MF до нелинейной формы, которая может быть более выразительной чем линейная MF модель. GMF может быть записана как:\n",
    "\n",
    "$$\\hat { r } _ { u , i } = a _ { o u t } \\left( h ^ { T } \\left( q _ { i } \\odot p _ { u } \\right) \\right)$$\n",
    "\n",
    "где $\\odot$ - поэлементное произведение векторов, ${a}_{out}$ и ${h}$ обозначают функцию активации и веса ребер выходного слоя соответственно. MF может рассматриваться как частный случай GMF. Интуитивно, если мы используем тождественную функцию для ${a}_{out}$ и выбираем единичный вектор в качестве ${h}$, то мы в точности повторяем модель MF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Модель MLP\n",
    "\n",
    "NCF использует два способа при моделировании рейтингов:\n",
    "\n",
    "1) поэлементное произведение векторов,\n",
    "2) конкатенация векторов.\n",
    "\n",
    "Сразу после контатенации скрытых признаков пользователей и айтемов применяется стандартная модель MLP. Это дает возможность наделить модель большим уровнем гибкости и нелинейности для изучения взаимодействий между $p_{u}$ и $q_{i}$. \n",
    "\n",
    "Запишем модель MLP модель более строго:\n",
    "\n",
    "Для входного слоя, используется конкатенация векторов пользователей и айтемов:\n",
    "\n",
    "$$z _ { 1 } = \\phi _ { 1 } \\left( p _ { u } , q _ { i } \\right) = \\left[ \\begin{array} { c } { p _ { u } } \\\\ { q _ { i } } \\end{array} \\right]$$\n",
    "\n",
    "Для скрытых и выходного слоев MLP запись имеет вид:\n",
    "\n",
    "$$\n",
    "\\phi _ { l } \\left( z _ { l } \\right) = a _ { o u t } \\left( W _ { l } ^ { T } z _ { l } + b _ { l } \\right) , ( l = 2,3 , \\ldots , L - 1 )\n",
    "$$\n",
    "\n",
    "и:\n",
    "\n",
    "$$\n",
    "\\hat { r } _ { u , i } = \\sigma \\left( h ^ { T } \\phi \\left( z _ { L - 1 } \\right) \\right)\n",
    "$$\n",
    "\n",
    "где ${ W }_{ l }$, ${ b }_{ l }$, и ${ a }_{ out }$ обозначают матрицу весов, вектор свободных членов и функцию активации для $l$-ого слоя, соответственно. В качестве функций активации MLP слоев, мы вольны выбирать любую: сигмоиду, гиперболический тангенс, ReLU и другие. В качестве функции активации на выходном слое используется сигмоида $\\sigma(x)=\\frac{1}{1+e^{-x}}$, чтобы ограничить оценки диапазоном (0,1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Смешивание GMF и MLP\n",
    "\n",
    "Чтобы обеспечить большую гибкость нашей смешанной модели мы позволяем GMF и MLP обучать независимые эмбединги и затем комбинируем две модели объединяя их последние скрытие слои. Мы взяли $\\phi^{GMF}$ из GMF:\n",
    "\n",
    "$$\\phi _ { u , i } ^ { G M F } = p _ { u } ^ { G M F } \\odot q _ { i } ^ { G M F }$$\n",
    "\n",
    "и получили $\\phi^{MLP}$ из MLP:\n",
    "\n",
    "$$\\phi _ { u , i } ^ { M L P } = a _ { o u t } \\left( W _ { L } ^ { T } \\left( a _ { o u t } \\left( \\ldots a _ { o u t } \\left( W _ { 2 } ^ { T } \\left[ \\begin{array} { c } { p _ { u } ^ { M L P } } \\\\ { q _ { i } ^ { M L P } } \\end{array} \\right] + b _ { 2 } \\right) \\ldots \\right) \\right) + b _ { L }\\right.$$\n",
    "\n",
    "Наконец, мы смешали выходы из GMF и MLP:\n",
    "\n",
    "$$\\hat { r } _ { u , i } = \\sigma \\left( h ^ { T } \\left[ \\begin{array} { l } { \\phi ^ { G M F } } \\\\ { \\phi ^ { M L P } } \\end{array} \\right] \\right)$$\n",
    "\n",
    "Модель сочетает линейность MF и нелинейность DNN при моделировании скрытых user–item структур."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 Целевая функция\n",
    "\n",
    "Мы можем записать функцию правдоподобия как:\n",
    "\n",
    "$$P \\left( \\mathcal { R } , \\mathcal { R } ^ { - } | \\mathbf { P } , \\mathbf { Q } , \\Theta \\right) = \\prod _ { ( u , i ) \\in \\mathcal { R } } \\hat { r } _ { u , i } \\prod _ { ( u , j ) \\in \\mathcal { R } ^{ - } } \\left( 1 - \\hat { r } _ { u , j } \\right)$$\n",
    "\n",
    "Где $\\mathcal{R}$ обозначает множество наблюдаемых взаимодействий пользователя, а $\\mathcal{ R } ^ { - }$ обозначает множество негативных наблюдений. $\\mathbf{P}$ и $\\mathbf{Q}$ - это скрытая матрица признаков пользователей и айтемов соответственно, $\\Theta$ - параметры модели. Взяв со знаком минус логарифм от правдоподобия мы получим целевую функцию для минимизации NCF алгоритма. Что-то напоминает, не правда ли? https://en.wikipedia.org/wiki/Cross_entropy\n",
    "\n",
    "$$L = - \\sum _ { ( u , i ) \\in \\mathcal { R } \\cup { \\mathcal { R } } ^ { - } } r _ { u , i } \\log \\hat { r } _ { u , i } + \\left( 1 - r _ { u , i } \\right) \\log \\left( 1 - \\hat { r } _ { u , i } \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NCF(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        user_num,\n",
    "        item_num,\n",
    "        factor_num,\n",
    "        num_layers,\n",
    "        dropout,\n",
    "        model,\n",
    "        GMF_model=None,\n",
    "        MLP_model=None,\n",
    "    ):\n",
    "        super(NCF, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        user_num: number of users;\n",
    "        item_num: number of items;\n",
    "        factor_num: number of predictive factors;\n",
    "        num_layers: the number of layers in MLP model;\n",
    "        dropout: dropout rate between fully connected layers;\n",
    "        model: 'MLP', 'GMF', 'NeuMF-end', and 'NeuMF-pre';\n",
    "        GMF_model: pre-trained GMF weights;\n",
    "        MLP_model: pre-trained MLP weights.\n",
    "        \"\"\"\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.model = model\n",
    "        self.GMF_model = GMF_model\n",
    "        self.MLP_model = MLP_model\n",
    "\n",
    "        self.embed_user_GMF = nn.Embedding(user_num, factor_num)\n",
    "        self.embed_item_GMF = nn.Embedding(item_num, factor_num)\n",
    "\n",
    "        self.embed_user_MLP = nn.Embedding(\n",
    "            user_num, factor_num * (2 ** (num_layers - 1))\n",
    "        )\n",
    "        self.embed_item_MLP = nn.Embedding(\n",
    "            item_num, factor_num * (2 ** (num_layers - 1))\n",
    "        )\n",
    "\n",
    "        MLP_modules = []\n",
    "        for i in range(num_layers):\n",
    "            input_size = factor_num * (2 ** (num_layers - i))\n",
    "            MLP_modules.append(nn.Dropout(p=self.dropout))\n",
    "            MLP_modules.append(nn.Linear(input_size, input_size // 2))\n",
    "            MLP_modules.append(nn.ReLU())\n",
    "        self.MLP_layers = nn.Sequential(*MLP_modules)\n",
    "\n",
    "        if self.model in [\"MLP\", \"GMF\"]:\n",
    "            predict_size = factor_num\n",
    "        else:\n",
    "            predict_size = factor_num * 2\n",
    "        self.predict_layer = nn.Linear(predict_size, 1)\n",
    "\n",
    "        self._init_weight_()\n",
    "\n",
    "    def _init_weight_(self):\n",
    "\n",
    "        \"\"\"We leave the weights initialization here.\"\"\"\n",
    "\n",
    "        if not self.model == \"NeuMF-pre\":\n",
    "            nn.init.normal_(self.embed_user_GMF.weight, std=0.01)\n",
    "            nn.init.normal_(self.embed_user_MLP.weight, std=0.01)\n",
    "            nn.init.normal_(self.embed_item_GMF.weight, std=0.01)\n",
    "            nn.init.normal_(self.embed_item_MLP.weight, std=0.01)\n",
    "\n",
    "            for m in self.MLP_layers:\n",
    "                if isinstance(m, nn.Linear):\n",
    "                    nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "            nn.init.kaiming_uniform_(\n",
    "                self.predict_layer.weight, a=1, nonlinearity=\"sigmoid\"\n",
    "            )\n",
    "\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "        else:\n",
    "            # embedding layers\n",
    "            self.embed_user_GMF.weight.data.copy_(self.GMF_model.embed_user_GMF.weight)\n",
    "            self.embed_item_GMF.weight.data.copy_(self.GMF_model.embed_item_GMF.weight)\n",
    "            self.embed_user_MLP.weight.data.copy_(self.MLP_model.embed_user_MLP.weight)\n",
    "            self.embed_item_MLP.weight.data.copy_(self.MLP_model.embed_item_MLP.weight)\n",
    "\n",
    "            # mlp layers\n",
    "            for (m1, m2) in zip(self.MLP_layers, self.MLP_model.MLP_layers):\n",
    "                if isinstance(m1, nn.Linear) and isinstance(m2, nn.Linear):\n",
    "                    m1.weight.data.copy_(m2.weight)\n",
    "                    m1.bias.data.copy_(m2.bias)\n",
    "\n",
    "            # predict layers\n",
    "            predict_weight = torch.cat(\n",
    "                [\n",
    "                    self.GMF_model.predict_layer.weight,\n",
    "                    self.MLP_model.predict_layer.weight,\n",
    "                ],\n",
    "                dim=1,\n",
    "            )\n",
    "\n",
    "            precit_bias = (\n",
    "                self.GMF_model.predict_layer.bias + self.MLP_model.predict_layer.bias\n",
    "            )\n",
    "\n",
    "            self.predict_layer.weight.data.copy_(0.5 * predict_weight)\n",
    "            self.predict_layer.bias.data.copy_(0.5 * precit_bias)\n",
    "\n",
    "    def forward(self, user, item):\n",
    "\n",
    "        if not self.model == \"MLP\":\n",
    "            embed_user_GMF = self.embed_user_GMF(user)\n",
    "            embed_item_GMF = self.embed_item_GMF(item)\n",
    "            output_GMF = embed_user_GMF * embed_item_GMF\n",
    "\n",
    "        if not self.model == \"GMF\":\n",
    "            embed_user_MLP = self.embed_user_MLP(user)\n",
    "            embed_item_MLP = self.embed_item_MLP(item)\n",
    "            interaction = torch.cat((embed_user_MLP, embed_item_MLP), -1)\n",
    "            output_MLP = self.MLP_layers(interaction)\n",
    "\n",
    "        if self.model == \"GMF\":\n",
    "            concat = output_GMF\n",
    "        elif self.model == \"MLP\":\n",
    "            concat = output_MLP\n",
    "        else:\n",
    "            concat = torch.cat((output_GMF, output_MLP), -1)\n",
    "\n",
    "        prediction = self.predict_layer(concat)\n",
    "        return prediction.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NCF(\n",
    "    user_num, item_num, factor_num=32, num_layers=3, dropout=0.0, model=\"NeuMF-end\"\n",
    ")\n",
    "model.to(DEVICE)\n",
    "\n",
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "loss_val = []\n",
    "\n",
    "for epoch in range(1):  # try more epoches\n",
    "\n",
    "    # Train\n",
    "    model.train()  # Enable dropout (if have).\n",
    "    train_loader.dataset.ng_sample()\n",
    "\n",
    "    for user, item, label in train_loader:\n",
    "        user = user.to(DEVICE)\n",
    "        item = item.to(DEVICE)\n",
    "        label = label.float().to(DEVICE)\n",
    "\n",
    "        model.zero_grad()\n",
    "        prediction = model(user, item)\n",
    "        loss = loss_function(prediction, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Plot learning curve\n",
    "        loss_val.append(loss.cpu().detach().numpy())\n",
    "\n",
    "        if len(loss_val) % 100 != 0:\n",
    "            continue\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(loss_val)\n",
    "        plt.xlabel(\"Number of iterations\", size=14)\n",
    "        plt.ylabel(\"Loss\", size=14)\n",
    "        plt.title(\"Epoch number: \" + str(epoch + 1), size=14)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Метрики качества"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Метрики качества предсказания событий:**\n",
    "\n",
    "$$Hitrate@k = |R_u (k) \\cap L_u|$$\n",
    "\n",
    "$$Precision@k = \\frac{|R_u (k) \\cap L_u|}{|R_u (k)|}$$\n",
    "\n",
    "$$Recall@k = \\frac{|R_u (k) \\cap L_u|}{|L_u|}$$\n",
    "\n",
    "где $R_u$ (k) - список k лучших рекомендаций алгоритма; $L_u$ - список фильмов, которые пользователь реально посмотрел.\n",
    "\n",
    "\n",
    "**Метрики качества ранжирования:**\n",
    "\n",
    "$$ DCG@k =  \\sum_{p=1}^{k} g(r_{ui_{p}}) d(p) $$\n",
    "\n",
    "где $p$ - позиция фильма в списке рекомендаций; $g(r) = 2^r - 1$; $d(p) = \\frac{1}{\\log(p+1)}$.\n",
    "\n",
    "$$ nDCG@k =  \\frac{DCG@k}{max DCG@k}$$\n",
    "\n",
    "\n",
    "**Конспект**: https://github.com/hse-ds/iad-applied-ds/blob/master/2020/lectures/lecture03-recommender.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1\n",
    "\n",
    "Напишите функцию для подсчета `Hitrate@k`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hit(gt_item, pred_items):\n",
    "    \"\"\"\n",
    "    gt_item : ID фильма, который пользователь реально посмотрел.\n",
    "    pred_items: спискок К рекомендаций фильмов.\n",
    "\n",
    "    Пример:\n",
    "    gt_item = 25\n",
    "    pred_items = [128, 25, 174, 273, 175, 1135, 1182, 617, 58, 1902]\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE ####\n",
    "    return 0\n",
    "    ### THE END ###########"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2\n",
    "\n",
    "Напишите функцию для подсчета `nDCG@k`. Используйте $g(r) = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ndcg(gt_item, pred_items):\n",
    "    \"\"\"\n",
    "    gt_item : ID фильма, который пользователь реально посмотрел.\n",
    "    pred_items: спискок К рекомендаций фильмов.\n",
    "\n",
    "    Пример:\n",
    "    gt_item = 25\n",
    "    pred_items = [128, 25, 174, 273, 175, 1135, 1182, 617, 58, 1902]\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE ####\n",
    "    return 0\n",
    "    ### THE END ###########"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 3\n",
    "\n",
    "Напишите функцию, которая будет считать средние значения метрик качесвта на тестовых данных. Используйте `torch.topk()` и `torch.take()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(model, test_loader, top_k):\n",
    "    HR, NDCG = [], []\n",
    "\n",
    "    for user, item, label in test_loader:\n",
    "\n",
    "        ### YOUR CODE HERE ####\n",
    "        HR.append(0)\n",
    "        NDCG.append(0)\n",
    "        ### THE END ###########\n",
    "\n",
    "    return np.mean(HR), np.mean(NDCG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем метрики качества на тесте."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "HR, NDCG = metrics(model, test_loader, top_k=10)\n",
    "HR, NDCG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Factorization Machine (FM)\n",
    "\n",
    "Авторы статьи https://arxiv.org/abs/2005.09683 утверждают, что NCF плохо учит произведение векторов. В результате, более простые методы демонстрируют лучшее качество рекомендаций. Сравним NCF c FM.\n",
    "\n",
    "<img src=\"https://s3-us-west-1.amazonaws.com/sijunhe-blog/plots/post15/fm_example.png\">\n",
    "\n",
    "Модель предполагает, что рейтинг $\\hat{y}(x)$ зависит от вектора признаков следующим образом:\n",
    "$$\\hat{y}(x) = w_0 + \\sum_{i=1}^{n}w_i x_i + \\sum_{i=1}^{n} \\sum_{j=i+1}^{n} <v_i, v_j> x_i x_j$$\n",
    "\n",
    "**Конспект:** https://github.com/hse-ds/iad-applied-ds/blob/master/2020/lectures/lecture02-recommender.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install git+https://github.com/coreylynch/pyFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyfm import pylibfm\n",
    "from sklearn.feature_extraction import DictVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка данных\n",
    "Приведем обучающую выборку к нужному формату."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_fm = []\n",
    "y_train = []\n",
    "\n",
    "for i in range(\n",
    "    0, len(train_dataset.features_fill), 10\n",
    "):  # весь датасет может сломать pyfm !!!\n",
    "    user, item = train_dataset.features_fill[i]\n",
    "    label = train_dataset.labels_fill[i]\n",
    "    train_dataset_fm += [{\"user\": str(user), \"item\": str(item)}]\n",
    "    y_train += [label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# пример\n",
    "train_dataset_fm[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# пример\n",
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение FM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# переведем словарь в метрицу векторов x\n",
    "v = DictVectorizer()\n",
    "X_train = v.fit_transform(train_dataset_fm)\n",
    "y_train = np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# обучение\n",
    "fm = pylibfm.FM(\n",
    "    num_factors=32,\n",
    "    num_iter=20,\n",
    "    verbose=True,\n",
    "    task=\"classification\",\n",
    "    initial_learning_rate=0.01,\n",
    "    learning_rate_schedule=\"optimal\",\n",
    ")\n",
    "fm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Измерение качества"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 4\n",
    "\n",
    "Напишите функцию, которая будет считать средние значения метрик качесвта на тестовых данных. Используйте `torch.topk()` и `torch.take()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metricsFM(model, test_loader, top_k):\n",
    "    HR, NDCG = [], []\n",
    "\n",
    "    for user, item, label in test_loader:\n",
    "\n",
    "        ### YOUR CODE HERE ####\n",
    "        HR.append(0)\n",
    "        NDCG.append(0)\n",
    "        ### THE END ###########\n",
    "\n",
    "    return np.mean(HR), np.mean(NDCG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HR, NDCG = metricsFM(fm, test_loader, top_k=10)\n",
    "HR, NDCG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Вопросы:\n",
    "- Какой из алгоритмов лучше?\n",
    "- Как вы думаете почему?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ссылки\n",
    "\n",
    "- Тетрадка реализовна на основе кода https://github.com/guoyang9/NCF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
