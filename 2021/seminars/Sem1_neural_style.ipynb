{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qFkxsrNw3oJb"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RgNljUK63oJh"
   },
   "source": [
    "\n",
    "Задача переноса стиля на изображении\n",
    "=============================\n",
    "\n",
    "За основу данного семинара взят ноутбук https://pytorch.org/tutorials/advanced/neural_style_tutorial.html \n",
    "\n",
    "Введение\n",
    "------------\n",
    "\n",
    "В этом ноутбуке мы реализуем алгоритм переноса стиля из статьи https://arxiv.org/abs/1508.06576 Leon A. Gatys, Alexander S. Ecker, Matthias Bethge.\n",
    "\n",
    "Перенос стиля (Neural-Style, Neural-Transfer) --- это процесс преобразования стиля исходного изображения к стилю выбранного изображения.\n",
    "\n",
    "***Основной идеей алгоритма, описанного в ноутбуке, является то, что\n",
    "признаки, полученные с помощью сверточной нейронной сети, можно использовать\n",
    "для выделения содержательной и стилевой составляющих изображения.***\n",
    "\n",
    "На вход алгоритму требуется три изображения:\n",
    "* input-image - изменяемое входное изображение (обычно берется исходное изображение или белый шум)\n",
    "* content-image - исходное изображение\n",
    "\n",
    "* style-image - изображение, стиль которого будет использован\n",
    "\n",
    "\n",
    "Алгоритм изменяет input-image таким образом, чтобы содержание было похоже на content-image, а стиль - на style-image.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B7dQ4off3oJi"
   },
   "source": [
    "Формализация идеи\n",
    "--------------------\n",
    "\n",
    "Мы определяем два расстояния (метрики):\n",
    "* $D_C$ - расстояние между содержанием на изображениях (content distance)\n",
    "* $D_S$ - расстояние между стилями на изображениях (style distance)\n",
    "\n",
    "Затем берём изменяемое изображение (input-image) и трансформируем его так, чтобы на результирующем изображении минимизровались оба расстояния.\n",
    "\n",
    "\n",
    "Импортируем необходимые библиотеки\n",
    "-----------------------------------------\n",
    "\n",
    "-  ``torch``, ``torch.nn``, ``numpy``\n",
    "-  ``torch.optim``\n",
    "-  ``PIL``, ``PIL.Image``, ``matplotlib.pyplot`` (загрузка и визуализация изображений)\n",
    "-  ``torchvision.transforms`` (трансформирует PIL-изображения в тензоры)\n",
    "-  ``torchvision.models`` \n",
    "-  ``copy`` \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jDzhLWEa3oJi"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import copy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wajvcmfp3oJi"
   },
   "source": [
    "Если возможно - запускаем алгоритм на GPU для ускорения вычислений:\n",
    "``torch.cuda.is_available()``\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "izbOqdNz3oJj"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e7e3IFF-3oJj"
   },
   "source": [
    "Загружаем изображения\n",
    "\n",
    "------------------\n",
    "\n",
    "Загружаем стилевое изображение и изменяемое (content) изображение.\n",
    "\n",
    "Исходные PIL-картинки содержат значения от 0 до 255, однако при их преобразовании в тензоры значения переходят на отрезок от 0 до 1.\n",
    "\n",
    "Также изображения должны быть приведены к одинаковым размерам.\n",
    "\n",
    "***Важно!*** В pytorch значения тензоров должны лежать на отрезке от 0 до 1. Если попробовать применить модель к тензору со значениями от 0 до 255, то в результате применения свёрток (feature maps) не получится корректно выделить содержание и стиль на изображении."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B-QmDVPn3oJj",
    "outputId": "df50a4ff-868d-4112-f9b0-a2451e72fa55"
   },
   "outputs": [],
   "source": [
    "# desired size of the output image\n",
    "imsize = 512 if torch.cuda.is_available() else 128  # use small size if no gpu\n",
    "\n",
    "loader = transforms.Compose(\n",
    "    [transforms.Resize(imsize), transforms.ToTensor()]  # scale imported image\n",
    ")  # transform it into a torch tensor\n",
    "\n",
    "\n",
    "def image_loader(image_name):\n",
    "    image = Image.open(image_name)\n",
    "    # fake batch dimension required to fit network's input dimensions\n",
    "    image = loader(image).unsqueeze(0)\n",
    "    return image.to(device, torch.float)\n",
    "\n",
    "\n",
    "!wget https://pytorch.org/tutorials/_static/img/neural-style/picasso.jpg\n",
    "!wget https://pytorch.org/tutorials/_static/img/neural-style/dancing.jpg\n",
    "\n",
    "style_img = image_loader(\"picasso.jpg\")\n",
    "content_img = image_loader(\"dancing.jpg\")\n",
    "\n",
    "assert (\n",
    "    style_img.size() == content_img.size()\n",
    "), \"we need to import style and content images of the same size\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tGPfyFiO3oJk"
   },
   "source": [
    "Создадим функцию, которая визуализирует изображение, копируя его и переводя обратно в PIL формат.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    },
    "id": "zx7oDIRS3oJk",
    "outputId": "76fa2d6d-497c-4247-8dff-84b9b0a186b0"
   },
   "outputs": [],
   "source": [
    "unloader = transforms.ToPILImage()  # reconvert into PIL image\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "\n",
    "def imshow(tensor, title=None):\n",
    "    image = tensor.cpu().clone()  # we clone the tensor to not do changes on it\n",
    "    image = image.squeeze(0)  # remove the fake batch dimension\n",
    "    image = unloader(image)\n",
    "    plt.imshow(image)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "imshow(style_img, title=\"Style Image\")\n",
    "\n",
    "plt.figure()\n",
    "imshow(content_img, title=\"Content Image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6h8Q_7--3oJk"
   },
   "source": [
    "Функции потерь (Loss)\n",
    "--------------\n",
    "\n",
    "***Content Loss***\n",
    "\n",
    "Content loss - взвешенный вариант content distance ($D_C$) для каждого свёрточного слоя сети.\n",
    "* принимает на вход feature maps $F_{XL}$ для изображения $X$ и слоя $L$ сети\n",
    "* возвращает взвешенное рассстояние $w_{CL}.D_C^L(X,C)$ между изображением $X$ и контентным изображением $C$.\n",
    "\n",
    "Feature maps контентного изображения ($F_{CL}$) должны быть известны заранее для вычисления контентного расстояния:\n",
    "$$D_C^L(X,C)=\\|F_{XL} - F_{CL}\\|^2$$ (для вычисления используем ``nn.MSELoss``).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cdpWcg8_3oJl"
   },
   "outputs": [],
   "source": [
    "class ContentLoss(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        target,\n",
    "    ):\n",
    "        super(ContentLoss, self).__init__()\n",
    "        # we 'detach' the target content from the tree used\n",
    "        # to dynamically compute the gradient: this is a stated value,\n",
    "        # not a variable. Otherwise the forward method of the criterion\n",
    "        # will throw an error.\n",
    "        self.target = target.detach()\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.loss = F.mse_loss(input, self.target)\n",
    "        return input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uXosdrWs3oJm"
   },
   "source": [
    "**Style Loss**\n",
    "\n",
    "Модуль Style loss выглядит похожим образом.\n",
    "\n",
    "Чтобы вычислить style loss, необходимо вычислить матрицу Грама (матрицу скалярных произведений) $G_{XL}$. Матрица Грама - это результат умножения матрицы на её транспонированный вариант. \n",
    "\n",
    "В данном случае нам нужна матрица feature maps $F_{XL}$ для слоя $L$. Мы трансформируем матрицу $F_{XL}$, чтобы она имела размерность $K \\times N$, где \n",
    "* $K$ - число feature maps на слое $L$\n",
    "* $N$ - длина каждой вытянутой в вектор feature map $F_{XL}^k$.\n",
    "Обозначим матрицу требуемой размерности $\\hat{F}_{XL}$.\n",
    "\n",
    "**Пояснение:**\n",
    "например, первая строка матрицы $\\hat{F}_{XL}$ соответствует первой feature map $F_{XL}^1$, вытянутой в вектор.\n",
    "\n",
    "Наконец, мы нормализуем матрицу Грама путем деления каждого элемента на число элементов матрицы. \n",
    "\n",
    "**Зачем это нужно?**\n",
    "Такая нормализация нужна потому, что матрицы $\\hat{F}_{XL}$ с большой размерностью $N$ приводят к большим значениям в матрице Грама. Большие значения в свою очередь могут спровоцировать то, что первые сверточные слои (до применения pooling) будут иметь наибольший вклад в процесс градиентного спуска. Но стилевые признаки по большей части содержатся в глубоких слоях нейронной сети, поэтому эта нормализация важна."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CW6WofyC3oJn"
   },
   "outputs": [],
   "source": [
    "def gram_matrix(input):\n",
    "    a, b, c, d = input.size()  # a=batch size(=1)\n",
    "    # b=number of feature maps\n",
    "    # (c,d)=dimensions of a f. map (N=c*d)\n",
    "\n",
    "    features = input.view(a * b, c * d)  # resise F_XL into \\hat F_XL\n",
    "\n",
    "    G = torch.mm(features, features.t())  # compute the gram product\n",
    "\n",
    "    # we 'normalize' the values of the gram matrix\n",
    "    # by dividing by the number of element in each feature maps.\n",
    "    return G.div(a * b * c * d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iF1WmBk-3oJo"
   },
   "source": [
    "Style loss, аналогично content loss, вычисляется через квадрат расстояния между $G_{XL}$ and $G_{SL}$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2BoLEzjo3oJp"
   },
   "outputs": [],
   "source": [
    "class StyleLoss(nn.Module):\n",
    "    def __init__(self, target_feature):\n",
    "        super(StyleLoss, self).__init__()\n",
    "        self.target = gram_matrix(target_feature).detach()\n",
    "\n",
    "    def forward(self, input):\n",
    "        G = gram_matrix(input)\n",
    "        self.loss = F.mse_loss(G, self.target)\n",
    "        return input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WGQUQK3m3oJp"
   },
   "source": [
    "Импортируем модель\n",
    "-------------------\n",
    "\n",
    "Будем использовать предобученную нейронную сеть. Для этой задачи возьмём VGG19.\n",
    "\n",
    "VGG в имплементации pytorch имеет две части:\n",
    "* ``Sequential`` modules: ``features`` (содержит свёртки и пулинги)\n",
    "* ``Classifier`` (содержит полносвязные слои). \n",
    "\n",
    "Мы будем пользоваться модулем ``features``, так как нам нужны результаты применения свёрток для вычисления контентного и стилевого лоссов.\n",
    "\n",
    "Так как мы используем предобученную нейронную сеть и дообучать мы её не планируем, то мы ставим сеть в режим применения: ``.eval()``.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FFwOKIZX3oJq"
   },
   "outputs": [],
   "source": [
    "cnn = models.vgg19(pretrained=True).features.to(device).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n0wEb4gz3oJq"
   },
   "source": [
    "VGG сети обучены на нормализованных изображениях с параметрами mean=[0.485, 0.456, 0.406] и std=[0.229, 0.224, 0.225].\n",
    "Поэтому перед подачей изображения в сеть его необходимо нормализовать таким же образом.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0i0tQu9o3oJq"
   },
   "outputs": [],
   "source": [
    "cnn_normalization_mean = torch.tensor([0.485, 0.456, 0.406]).to(device)\n",
    "cnn_normalization_std = torch.tensor([0.229, 0.224, 0.225]).to(device)\n",
    "\n",
    "# create a module to normalize input image so we can easily put it in a\n",
    "# nn.Sequential\n",
    "class Normalization(nn.Module):\n",
    "    def __init__(self, mean, std):\n",
    "        super(Normalization, self).__init__()\n",
    "        # .view the mean and std to make them [C x 1 x 1] so that they can\n",
    "        # directly work with image Tensor of shape [B x C x H x W].\n",
    "        # B is batch size. C is number of channels. H is height and W is width.\n",
    "        self.mean = torch.tensor(mean).view(-1, 1, 1)\n",
    "        self.std = torch.tensor(std).view(-1, 1, 1)\n",
    "\n",
    "    def forward(self, img):\n",
    "        # normalize img\n",
    "        return (img - self.mean) / self.std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lM47IsfQ3oJq"
   },
   "source": [
    "Теперь мы должны добавить к свёрточной части сети VGG19 стилевой и контентный лосс сразу после свёрточных слоёв.\n",
    "\n",
    "Для этого необходимо создать новый модуль ``Sequential`` с добавленными в нужные места лоссами.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o2D-obcN3oJr"
   },
   "outputs": [],
   "source": [
    "# desired depth layers to compute style/content losses :\n",
    "content_layers_default = [\"conv_4\"]\n",
    "style_layers_default = [\"conv_1\", \"conv_2\", \"conv_3\", \"conv_4\", \"conv_5\"]\n",
    "\n",
    "\n",
    "def get_style_model_and_losses(\n",
    "    cnn,\n",
    "    normalization_mean,\n",
    "    normalization_std,\n",
    "    style_img,\n",
    "    content_img,\n",
    "    content_layers=content_layers_default,\n",
    "    style_layers=style_layers_default,\n",
    "):\n",
    "    cnn = copy.deepcopy(cnn)\n",
    "\n",
    "    # normalization module\n",
    "    normalization = Normalization(normalization_mean, normalization_std).to(device)\n",
    "\n",
    "    # just in order to have an iterable access to or list of content/syle\n",
    "    # losses\n",
    "    content_losses = []\n",
    "    style_losses = []\n",
    "\n",
    "    # assuming that cnn is a nn.Sequential, so we make a new nn.Sequential\n",
    "    # to put in modules that are supposed to be activated sequentially\n",
    "    model = nn.Sequential(normalization)\n",
    "\n",
    "    i = 0  # increment every time we see a conv\n",
    "    for layer in cnn.children():\n",
    "        if isinstance(layer, nn.Conv2d):\n",
    "            i += 1\n",
    "            name = \"conv_{}\".format(i)\n",
    "        elif isinstance(layer, nn.ReLU):\n",
    "            name = \"relu_{}\".format(i)\n",
    "            # The in-place version doesn't play very nicely with the ContentLoss\n",
    "            # and StyleLoss we insert below. So we replace with out-of-place\n",
    "            # ones here.\n",
    "            layer = nn.ReLU(inplace=False)\n",
    "        elif isinstance(layer, nn.MaxPool2d):\n",
    "            name = \"pool_{}\".format(i)\n",
    "        elif isinstance(layer, nn.BatchNorm2d):\n",
    "            name = \"bn_{}\".format(i)\n",
    "        else:\n",
    "            raise RuntimeError(\n",
    "                \"Unrecognized layer: {}\".format(layer.__class__.__name__)\n",
    "            )\n",
    "\n",
    "        model.add_module(name, layer)\n",
    "\n",
    "        if name in content_layers:\n",
    "            # add content loss:\n",
    "            target = model(content_img).detach()\n",
    "            content_loss = ContentLoss(target)\n",
    "            model.add_module(\"content_loss_{}\".format(i), content_loss)\n",
    "            content_losses.append(content_loss)\n",
    "\n",
    "        if name in style_layers:\n",
    "            # add style loss:\n",
    "            target_feature = model(style_img).detach()\n",
    "            style_loss = StyleLoss(target_feature)\n",
    "            model.add_module(\"style_loss_{}\".format(i), style_loss)\n",
    "            style_losses.append(style_loss)\n",
    "\n",
    "    # now we trim off the layers after the last content and style losses\n",
    "    for i in range(len(model) - 1, -1, -1):\n",
    "        if isinstance(model[i], ContentLoss) or isinstance(model[i], StyleLoss):\n",
    "            break\n",
    "\n",
    "    model = model[: (i + 1)]\n",
    "\n",
    "    return model, style_losses, content_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dr1AySdi3oJs"
   },
   "source": [
    "Теперь выбираем input image. В качестве input image можно использовать, например, исходное (content) изображение или белый шум.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "UwWCyWu63oJs",
    "outputId": "32b2b739-b104-4bc9-ac84-d10c48197381"
   },
   "outputs": [],
   "source": [
    "input_img = content_img.clone()\n",
    "\n",
    "# add the original input image to the figure:\n",
    "plt.figure()\n",
    "imshow(input_img, title=\"Input Image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KsIi55qQ3oJs"
   },
   "source": [
    "Градиентный спуск\n",
    "----------------\n",
    "\n",
    "Как предложил автор алгоритма, мы будем использовать алгоритм\n",
    "L-BFGS градиентного спуска. \n",
    "\n",
    "В данной версии алгоритма мы обучаем не веса в сети, а исходное изображение, чтобы минимизировать content/style\n",
    "losses. Для этого используем pytorch ``optim.LBFGS`` и передаем наше изображение (как тензор) в оптимизатор.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TXpJ_Qrk3oJs"
   },
   "outputs": [],
   "source": [
    "def get_input_optimizer(input_img):\n",
    "    # this line to show that input is a parameter that requires a gradient\n",
    "    optimizer = optim.LBFGS([input_img.requires_grad_()])\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kIJ8MtHz3oJt"
   },
   "source": [
    "Наконец, создадим функцию, осуществляющую стилевой перенос.\n",
    "* на каждой итерации она получает на вход изображение и вычисляет новые значения лоссов (style/content).\n",
    "* также на каждом шаге с помощью ``backward`` методов динамически вычисляются градиенты функций потерь. \n",
    "\n",
    "Также необходимо привести значения тензоров к отрезку $[0;1]$ на каждой итерации.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4n03flqX3oJt"
   },
   "outputs": [],
   "source": [
    "def run_style_transfer(\n",
    "    cnn,\n",
    "    normalization_mean,\n",
    "    normalization_std,\n",
    "    content_img,\n",
    "    style_img,\n",
    "    input_img,\n",
    "    num_steps=300,\n",
    "    style_weight=1000000,\n",
    "    content_weight=1,\n",
    "):\n",
    "    \"\"\"Run the style transfer.\"\"\"\n",
    "    print(\"Building the style transfer model..\")\n",
    "    model, style_losses, content_losses = get_style_model_and_losses(\n",
    "        cnn, normalization_mean, normalization_std, style_img, content_img\n",
    "    )\n",
    "    optimizer = get_input_optimizer(input_img)\n",
    "\n",
    "    print(\"Optimizing..\")\n",
    "    run = [0]\n",
    "    while run[0] <= num_steps:\n",
    "\n",
    "        def closure():\n",
    "            # correct the values of updated input image\n",
    "            input_img.data.clamp_(0, 1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            model(input_img)\n",
    "            style_score = 0\n",
    "            content_score = 0\n",
    "\n",
    "            for sl in style_losses:\n",
    "                style_score += sl.loss\n",
    "            for cl in content_losses:\n",
    "                content_score += cl.loss\n",
    "\n",
    "            style_score *= style_weight\n",
    "            content_score *= content_weight\n",
    "\n",
    "            loss = style_score + content_score\n",
    "            loss.backward()\n",
    "\n",
    "            run[0] += 1\n",
    "            if run[0] % 50 == 0:\n",
    "                print(\"run {}:\".format(run))\n",
    "                print(\n",
    "                    \"Style Loss : {:4f} Content Loss: {:4f}\".format(\n",
    "                        style_score.item(), content_score.item()\n",
    "                    )\n",
    "                )\n",
    "                print()\n",
    "\n",
    "            return style_score + content_score\n",
    "\n",
    "        optimizer.step(closure)\n",
    "\n",
    "    # a last correction...\n",
    "    input_img.data.clamp_(0, 1)\n",
    "\n",
    "    return input_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TYNQr8hY3oJt"
   },
   "source": [
    "Запускаем алгоритм!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 717
    },
    "id": "sggDPXW33oJt",
    "outputId": "65f82e78-d368-4fb2-cd93-0c19af8c90a1"
   },
   "outputs": [],
   "source": [
    "output = run_style_transfer(\n",
    "    cnn,\n",
    "    cnn_normalization_mean,\n",
    "    cnn_normalization_std,\n",
    "    content_img,\n",
    "    test_img,\n",
    "    input_img,\n",
    "    style_weight=500000,\n",
    ")\n",
    "\n",
    "plt.figure()\n",
    "imshow(output, title=\"Output Image\")\n",
    "\n",
    "# sphinx_gallery_thumbnail_number = 4\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "198DjqpHOXmm"
   },
   "source": [
    "#### Задание 1. \n",
    "\n",
    "Мы запустили алгоритм, используя в качестве input_image исходное изображение (content_image). Попробуйте в качестве input_image взять белый шум. Визуализируйте результат."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FfJdvqAYOWyz"
   },
   "outputs": [],
   "source": [
    "# your code here (you can change an existing code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sknp42TJF5RU"
   },
   "source": [
    "#### Задание 2.\n",
    "Загрузите другую стилевую картинку (например, \"звездную ночь\" Ван Гога или любую другую), не забудьте привести её к тем же размерам, что и картинка с содержанием.\n",
    "\n",
    "Примените новый стиль к изображению. Нравится ли вам результат?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wvMyzJjuF3N5"
   },
   "outputs": [],
   "source": [
    "# your code here (you can change an existing code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fkye30ZBGvO_"
   },
   "source": [
    "#### Задание 3.\n",
    "Поэкспериментируйте с коэффициентами при style_loss и content_loss (по дефолту используются значения style_weight=1000000, content_weight=1).\n",
    "\n",
    "Выведите на экран итоговые изображения для различных по порядку style_weight (используйте значения весов в диапазоне от 0 до 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4APG85plF3Sg"
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9_ei7OjPHU-K"
   },
   "source": [
    "#### Задание 4.\n",
    "Для вычисления итогового style_loss мы суммируем style_loss по первым пяти свёрточным слоям, то есть каждый слой (из первых пяти) имеет одинаковый вклад в итоговый style_loss. Но можно вычислять итоговый style_loss, суммируя style_loss'ы по слоям с весами. \n",
    "\n",
    "Создайте список весов style_weights и запишите туда пять весов, которые по вашему мнению дают наибольшие артефакты стиля. Измените вычисление style_loss в процедуре run_style_transfer, домножая style_loss с каждого слоя на соответствующий вес. \n",
    "\n",
    "Проведите эксперименты для:\n",
    "* вашего массива style_weights\n",
    "* style_weights = [1, 0, 0, 0, 0]\n",
    "* style_weights = [0, 0, 0, 0, 1]\n",
    "\n",
    "Какие выводы можно сделать из этих экспериментов?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "34CQguepF3Ug"
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XehasL7iJMvC"
   },
   "source": [
    "#### Задание 5.\n",
    "Описанный в ноутбуке алгоритм работает достаточно медленно, особенно на изображениях большого размера. Как можно модифицировать алгоритм, чтобы снизить число обучаемых параметров?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gH-EyguBF3X7"
   },
   "outputs": [],
   "source": [
    "# your answer here"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Sem1_neural_style",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
